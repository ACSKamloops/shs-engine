# Pukaist / Codex Pipeline Environment (EXAMPLE)
# Copy to `.env` and edit locally. Do NOT commit secrets.

# -----------------------------
# Codex CLI execution settings
# -----------------------------

# Default Codex profile name (matches `~/.codex/config.toml [profiles.*]`)
PUKAIST_CODEX_PROFILE=pukaist_exec

# Primary model name.
# For local providers (Ollama/LM Studio), set this to the locally installed model id
# (e.g., llama3.2, deepseek-r1:14b, qwen2.5, etc.).
PUKAIST_CODEX_MODEL=llama3.2

# Optional extra flags passed to `codex exec`.
# Examples:
#   PUKAIST_CODEX_EXEC_FLAGS="--model gpt-5.2"
#   PUKAIST_CODEX_EXEC_FLAGS="--oss --local-provider ollama"
#   PUKAIST_CODEX_EXEC_FLAGS="-c model_provider='openai'"
PUKAIST_CODEX_EXEC_FLAGS=

# Optional: write a raw JSONL event log for each Codex exec run (audit trail).
# When set to 1/true, `codex_exec_runner.sh` adds `--json` and tees stdout to a file.
PUKAIST_CODEX_LOG_EVENTS=

# Where to store event logs (relative or absolute). Default is `99_Working_Files/Logs`.
PUKAIST_CODEX_LOG_DIR=99_Working_Files/Logs

# If you want to hard‑pin the sandbox/approvals for exec runs:
# PUKAIST_CODEX_SANDBOX=workspace-write
# PUKAIST_CODEX_APPROVAL_POLICY=never

# -----------------------------
# Provider API keys (optional)
# -----------------------------
# Codex CLI uses OpenAI/ChatGPT by default. These are placeholders for
# multi‑provider setups (e.g., via Codex model_provider, local providers,
# or external wrappers you add later). Leave blank if unused.

OPENAI_API_KEY=
OPENAI_ORG_ID=

GEMINI_API_KEY=

ANTHROPIC_API_KEY=

AZURE_OPENAI_API_KEY=
AZURE_OPENAI_ENDPOINT=
AZURE_OPENAI_DEPLOYMENT=

# Local provider endpoints (if using --oss / LM Studio / Ollama)
LMSTUDIO_BASE_URL=http://127.0.0.1:1234/v1
OLLAMA_BASE_URL=http://127.0.0.1:11434/v1

# -----------------------------
# Pipeline identity
# -----------------------------
# Stamp locks and logs with the active agent name
PUKAIST_AGENT=CodexExec
